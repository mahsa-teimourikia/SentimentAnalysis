{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Python\n",
    "\n",
    "Here the goal is to understand how positive or negative is the sentiment in texts, which are usually reviews/comments of some kind.\n",
    "\n",
    "The data is taken from this [link](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html) which includes Multi-Domain Sentiment Dataset. This dataset contains product reviews taken from Amazon.com from 4 product types (domains): Kitchen, Books, DVDs, and Electronics. Each domain has several thousand reviews, but the exact number varies by domain. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.\n",
    "\n",
    "These data are in XML format and therefore, we will need an XML parser to read them. Here we will use _BeautifulSoup_ as an XML parser. It can be installed with the following command:\n",
    "\n",
    "```\n",
    "conda install -c anaconda beautifulsoup4 \n",
    "```\n",
    "\n",
    "## Outline of the Sentiment Analyzer\n",
    "\n",
    "Here we will look at the electronics category. One approach would be doing regression using the 4 star targets, however, as the data is already labeled with \"positive\"/\"negative\" sentiments we will use a classification approach using a simple classifier like _logistic regression_ which also allows us to interpret the labels.\n",
    "\n",
    "For this purpose, we only need to look at the review text that can be found under the tag \"review_text\".\n",
    "\n",
    "Two passes are required here, one to determine the vocabulary size and corresponding indexes and words, and another for creating the data vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Review\n",
    "For a review on Logistic Regression theory and it's use in python you can refer to this tutorial: https://github.com/mahsa-teimourikia/LogisticRegressionWithPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is a process that splits an input sequence into so-called tokens which are units for semantic processing.\n",
    "\n",
    "NLTK library includes the tokenization functionality, here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Mahsa's\", 'cup,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is Mahsa's cup, isn't it?\"\n",
    "\n",
    "# Using the white space tokenizer\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Mahsa', \"'s\", 'cup', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting by a set of rules\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Mahsa', \"'\", 's', 'cup', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting by punctuation\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is required to write our own tokenizer, such as cases in which we require to remove the stop words, remove numbers, or single-letter tokens. Other cases are application specific cases that might require specific tokenization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
